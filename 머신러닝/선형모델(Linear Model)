지도학습 -분류-KNN,Decision Tree,RandomForest-Baging모델,Boost계열
         -회귀-Linear Regression,SGDRegressor,Lidge,Lasso
-선형모델LinearModel         
입력특성-data
절편-y축과 만나는 점
기울기=가중치=계수 같은말
절편=편향 같은 말
선형함수(예측값)-모든데이터를 반영할 수 있는 선 하나를 그어야한다.
비용함수(Cost function)-모든데이터를 아우를 수 있는 기준 검증하는 함수
비용함수를 통해서 모든데이터를 아우를 수 있는 함수를 만드는 기준을 만드는 것
비용함수- 예측값과 타깃값(실제값) 사이의 오차들의 합 사용
오차의 제곱 값 사용-오차에 따라서 양의 오차가 있고 음의 오차가 있기 때문에 오차를 다 더하면 오차들끼리 상쇄되서 내가 만든 예측함수에 대해 그대로 표현 할 수 없음 
제곱을 하면 오차들끼리 상쇄되지 않아서 내가 만든 예측함수에 표현 할 수 있음
비용함수가 작을 수록 잘 반영 클 수록 데이터가 반영이 안된거
평균제곱오차-비용함수
오차가 최소인 예측함수를 찾는 것이 목표
평균제곱오차-오차들의 제곱의 평균을 구한 것을 비용함수로 사용한다.
-------
수학적 공식을 이용한 해석적 방법(Ordinary Least Squeres)
LinearRegression 선형회귀
- 선형 모델을 사용해서 회귀 분석
- 수학적 공식을 이용한 모델
- 비용함수(평균제곱오차)가 최소가 되는 지점을 계산을 통해서 한번에 출력
- 출력값을 수정할 방법이 없기 때문에 과대/과소 적합을 해소 할 수 없음

LinearRegression + 정규화(규제) > 과대/과소적합을 해소
모델 정규화 - 과대적합에 대한 제어
-Lasso
-모든 원소에 똑같은 힘으로 규제를 가한다.
모든 모델에 똑같은 가중치를 적용함으로서 가중치 값이 작았던 애들은 중요하지 않다.
중요한 특성과 중요하지 않은 특성을 선택할 수 있다.
-Ridge
- 모든 원소에 골고루 규제를 가한다. 똑같은 퍼센트만큼 가하게 된다.(똑같은 비율만큼 제약)
- 가중치가 커지면 클 수록 제약을 많이 걸 수록 0이 되지 않는다.
- 즉 모든 원소를 다 사용하게 된다.
----경사하강법

---지금까지 요약
선형모델 
-입력 특성에 대해서 입력특성을 모두 반영하는 예측함수(선x)을 그어서 예측
- 예측함수 특성의 개수 -1만큼의 차원을 가짐

예측함수
-예측함수가 제대로 예측함수인지 어떻게 구분할 것인가?
-비용함수(함수를 검증): 평균 제곱오차(MSE)
+평균제곱오차가 가장 작은 예측함수가 입력특성을 모두 반영하는 예측함수

어떻게 평균제곱오차가 가장 작은 예측함수를 찾을 것인가?
- 수학적 공식> 수학적 공식을 통해서 평균제곱오차가 가장 작은 예측함수를 한 번에 찾음
- 경사하강법>임의의 한점에서부터 평균 제곱 오차가 가장 작은 점을 서서히 찾아감


---2021-06-22
alpha = 규제
규제가 커졌다 = 컬럼들의 사용량이 줄어든다. >학습량이 줄어든다>그래프 왼쪽으로 이동 
>과소적합이 걸릴 가능성이 커짐,과대적합을 피할 수 있다

규제가 작아진다 = 컬럼들의 사용량이 많아진다 >학습량이 늘어난다>그래프 오른쪽으로 이동>과소적합을 피할 수 있다,과대적합이 걸릴 가능성이 커짐

ridge,lasso = linearRegression + 규제

ridge와 lasso 사용시 규제를 매우 약하게 주면
linearRegression모델과 동일해짐(linearRegression과 똑같은 결과값을 출력하게 된다)

lasso는 특성선택이 가능하다
Ridge는 모든 원소에 골고루 규제를 적용하기 때문에 전체 컬럼의 개수가 고정이 되어 있다.      

MSE-평균제곱오차
RMSE-평균제곱근오차
MSE 와 RMSE는 값의 범위를 알 수 없다.
내가 100이라는 숫자가 나왔으면 얼마만큼 예측을 잘한지 알 수 가 없음

MSE 와 RMSE는 값의 범위가 무한정 늘어나기 때문에
좋은값과 나쁜값의 구분이 힘듬

따라서 평가지표로 R^2를 사용
- 값의 범위가 0 ~ 1
- 1로 나올수록 좋은 평가지표 = 확률값으로 사용
얼마만큼 예측했는지 알 수 있기 때문에 r-squared값을 사용한다.

어떤기준을 통해서 모델을 선택해야하는가?
Regression문제 판단 
대용량이 아니면 SGDRegressor(모델자체로 여러번 학습해서 최적의 파라미터를 찾아줌 -> 대용량을 하면 시간이 많이 걸리기때문에 대용량일때는 기피)
대용량이면 LinearRegression(수학적 공식으로 바로 한번에 찾아가기 때문에 대용량에 쓰면 좋다.)
만약 과대적합이 일어나면 어떤 특성이 중요할 것 같은가? 질문해야합니다.
일부 특성이 중요한 경우
Lasso(모든 특성에 똑같은 규제를 가해서 특정한 특성을 선택하는 lasso)
일부특성이 중요하지 않으면
Ridge(모든 특성을 고루고루 적용하기때문에)

분류용선형 모델
0보다 큰지 0보다 작은지로 판단함
일대다방법-여러개의 예측함수를 나눠서 여러개로 나눔
다중클래스 분류할때 쓰임

Logistic Regression
-> S자형 곡선을 사용함
자연현상같은 경우 직선보다 곡선으로 예측할 때 더 잘 맞춘다.

sigmoid Function 
직선을 곡선을 바꿔주는 함수 
0~1로 분류해서 바로 예측값으로 사용할 수 없음

회귀 - alpha
분류 - c를 통해서 규제를 하게 된다.
c값이 커지면 규제가 완화 c값이 작아지면 규제가 강황

SVM
support vector

f1-score=1에 가까울 수록 정밀도와 재현율이 잘 어우러졌다.
재현율을 높이면 애매한 것도 다 잡는다.
정밀도를 높이면 확실한 것만 잡는다.
애매한 것들을 어떻게 처리할 것인지 판단하는 지표

ROC curve 1에가까울록 좋다.
크게 사용되지 않음
macro avg는 다 더해서 나눈 평균

